%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lachaise Assignment
% LaTeX Template
% Version 1.0 (26/6/2018)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Marion Lachaise & François Févotte
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{algorithmicx}
\usepackage{algpseudocode}


\lstset{
	language=Python,
	basicstyle=\ttfamily\small,
	keywordstyle=\color{blue},
	stringstyle=\color{red},
	commentstyle=\color{purple},
	showstringspaces=false,
	numbers=left,
	numberstyle=\tiny\color{gray},
	breaklines=true,
	frame=single,
	captionpos=b
}
\newtheorem{definition}{Definition}[section]

\input{structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{The Maths Behind Denoising Diffusion Probabilistic Models} % Title of the assignment

\author{Giovani Tavares\\ \texttt{giovanitavares@outlook.com}} % Author name and email address

\date{University of Sao Paulo --- \today} % University, school and/or department name(s) and a date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section*{Motivation} % Unnumbered section

I've been studying and using generative AI models for a while now. I have always put at least some effort in understanding Large Language Model's (LLMs) architecture and training process, but state-of-the-art (SOTA) image generative models have always been kept as a future topic to be grasped. I say that the personal main reason for this is that I hadn't had the opportunity to use this models professionally until recently, so I had little interest in dedicating some time for them.

By the last months of 2024, I was invited by my boss to give a worshop on Generative Adversarial Networks (GANs) and Denoising Diffusion Probabilistic Models (DDPMs) during the Third School on Data Science and Machine Learning that took place in the International Centre for Theoretical Physics (ICTP) in the São Paulo State University (UNESP, Sao Paulo, Brazil) in December 6th, 2024. The workshop was my greatest academic challenge when it comes to presenting. The public was made of high performing graduate and postgraduate students from the best universities in South America. 

As mentioned, DDPMs was one of the topics I had to give a workshop on. In order to do so, I had to study its architecture and training procedure. The concepts I learned are all presented here.


\section{What are DDPMs?} % Numbered section

Denoising Diffusion Probabilistic Models (DDPMs) are  models capable of predicting \textit{noise} from a noisy input. By using such prediction, a sampling algorithm can be used to remove the noise from the input which results in a denoised output. DDPMs were first introduced in 2015 by Sohl-Dickstein and other USA researchers in \cite{DBLP:journals/corr/Sohl-DicksteinW15}, but were made popular by Jonathan Ho and others in \cite{DBLP:journals/corr/abs-2006-11239}. The latter paper used such model to generate images outputs from pure random noise, which is basically an image generation model.

DDPMs are made of two processes: a \textbf{forward} and a \textbf{reversion} process. The former is responsible for gradually adding noise to a  image by sampling from a normal distribution according to a Markov Chain. The latter removes  added noise by sampling from another normal distribution. In simple terms, the training of DDPMs involve learning the reversion process' distribution's parameters.


\subsection{Forward Process}

The process of adding noise to an input image ($x_0$) is a Markov Chain that generates a noisier image $x_t$ from a less noisy image $x_{t-1}$. In \cite{DBLP:journals/corr/abs-2006-11239} and here, an $x$'s  subscript represents its position in the Markov Chain so that the greater an $x$'s  subscript the noisier it is. Hence, $x_t$ represents the result of adding noise to $x_{t-1}$ by transitioning it once in the Markov Chain.


From the original DDPM paper, we know that in the forward process, a sample $x_t$ is produced by adding noise to a sample $x_{t-1}$ according to a normal distribution defined below:

\begin{align}
	q(x_t|x_{t-1}) := \mathcal{N}(x_{t}; \sqrt{1-\beta_t} \times x_{t-1}; \beta_tI)
\end{align}


Using $q(x_t|x_{t-1})$  is not very feasible from an implementation point of view, because it makes the production of each noisy sample $x_{t}$ dependent on $x_{t-1}$, which makes it necessary to make $t$ transitions starting from $x_0$ in order to get to the $x_t$ sample. \textbf{Ideally, one would need a single transition from $x_0$  to get to $x_t$, with $t>0$.}

The authors of \cite{DBLP:journals/corr/abs-2006-11239} achieves such ideal scenario by defining a cumulative noise $\alpha_t$ presented below:

\begin{definition}[Cumulative Noise]
	The Cumulative Noise ($\alpha_t$) added to an input  $x_0$ up to the t-th step is defined as:
		\label{def:cum_noise}
	\begin{align}
		\alpha_t &:= 1 - \beta_t \\
		\bar{\alpha_t} &:= \prod_{s=1}^{t} a_s \\
		\text{which leads to}\\
		q(x_t|x_0) &:= \mathcal{N}(x_{t}; \sqrt{\bar{\alpha_t}} \times x_0; (1 - \bar{\alpha_t})I)
	\end{align}
\end{definition}


Definition \ref{def:cum_noise} permits the sampling of $x_t$ to be done in a single transition from $x_0$.

According to the original DDPM paper, the \textbf{Variance Schedule} $\beta_1, ..., \beta_T$ sequence that defines the noisy images distribution can be learned or held constant as hyperparameters. The authors uses the latter approach, which makes the forward process' variances constant.

\subsection{Reverse  Process}

As previously mentioned, the reverse process is responsible for removing the noise from an input. Hence, this process is responsible for generating $\mathbf{x_0}$ given a noisy input $\mathbf{x_t}$. It is a \textit{Markov Chain} with learned Gaussian transitions defined below:


\begin{definition}[Reverse Process Transitions]
	The Reverse Process is a Markov Chain with the following transitions:
	\label{def:rev_process_trans}
	\begin{align}
		p(x_T) &= \mathcal{N}(x_T; 0; 1)\\
	    p_{\theta}(x_{0:T}) &:= p(x_T) \prod_{t=1}^{T} p_{\theta}(x_{t-1} | x_t) \\
	 	p_{\theta}(x_{t-1} | x_t) & := \mathcal{N}(x_{t-1}; \mu_\theta(x_t,t); \Sigma_\theta(x_t, t))
	\end{align}
\end{definition}

Notice that the definition above is simply a Markov Chain with a Standard Gaussian initial state and Gaussian transition distributions parametrized by $\mu_\theta$ and $\Sigma_\theta$. We want to create a model that is capable of recovering the best $x_0$ possible given a set of observed variables $Z := \mathbf{x_{1:T}}$. Theoretically, this can be achieved if we have a process to sample from the following distribution.

\begin{definition}[Reverse Process Prior]
	Ideally, the Reverse Process will let us sample from:
	\label{def:rev_process_post}
	\begin{align}
		p(\mathbf{x}_0) &=  \int p(\mathbf{x_0}, \mathbf{x_{1:T}})\mathbf{d_{1:T}} \\
	\end{align}
\end{definition}

We see that the computation of $p(\mathbf{x_0})$ is highly complex and intractable due to its multidimensionality, which makes the training based on optimizating $p(\mathbf{x_{0}})$ directly infeasible. To solve this problem, the author's of the DDPM paper used the \textit{Variational Lower Bound} of the expectation log-likelihood function, also called evidence lower bound (ELBO).

\subsubsection{Variational Lower Bound / Evidence Lower Bound / ELBO}

 The Variational Lower Bound is a tight lower bound that limits $log(p(\mathbf{x_0}))$ from below. Hence, when $p(\mathbf{x_0})$ is intractable as in the case of DDPMs, one can always maximize such lower bound as a means to ensure that $log(p(\mathbf{x_0}))$ is as large as possible. Such lower bound is often called \textbf{ELBO} or simply \textbf{$L$} and will be demonstrated using two different approaches: the \textbf{Jensen's Inequality} and the \textbf{KL Divergence}.
 The Variational Lower Bound is a tight lower bound that limits $log(p(\mathbf{x_0}))$ from below. Hence, when $p(\mathbf{x_0})$ is intractable as in the case of DDPMs, one can always maximize such lower bound as a means to ensure that $log(p(\mathbf{x_0}))$ is as large as possible. Such lower bound is often called \textbf{ELBO} or simply \textbf{$L$} and will be demonstrated using two different approaches: the \textbf{Jensen's Inequality} and the \textbf{KL Divergence}.
 

\begin{definition}[Variational Lower Bound - Jensen's Inequality]
	Let's use the Rule Of Total Probability to find an upper bound for the log-likelihood function.
	\label{def:var_low_bound_jensen}
	\begin{align}
		log [p_\theta(\mathbf{x_0})] &=  log \int_\mathbf{x_{1:T}} p(\mathbf{x_0}, \mathbf{x_{1:T}})d\mathbf{x_{1:T}} \\
		log [p_\theta(\mathbf{x_0})] &=  log \int_\mathbf{x_{1:T}} p(\mathbf{x_0}, \mathbf{x_{1:T}}) \frac{q(\mathbf{x_{1:T}| x_0})}{q(\mathbf{x_{1:T}| x_0})}d\mathbf{x_{1:T}} 	\\
		log [p_\theta(\mathbf{x_0})] &= log (\mathbb{E}_q\bigg[\frac{p(\mathbf{x_{0:T}})} {q(\mathbf{x_{1:T}| x_0})}\bigg]) \\
		\text{the Jensen's inequality tells us}& \\
		f(\mathbf{E}(\mathbf{X})) &\geq \mathbf{E}(f(\mathbf{X})))\\
		\text{   for any concave function f.} &\\
		\text{ log is concave, hence:} &  \\
		log [p_\theta(\mathbf{x_0})] &\geq \mathbb{E}_q\bigg[log \frac{p(\mathbf{x_{0:T}})} {q(\mathbf{x_{1:T}| x_0})}\bigg] \\
		\text{If we define the Variational Lower Bound L as:}  &\\
		L &:=   \mathbb{E}_q\bigg[log \frac{p(\mathbf{x_{0:T}})} {q(\mathbf{x_{1:T}| x_0})}\bigg] \\
		\implies - log [p_\theta(\mathbf{x_0})] &\leq - L
	\end{align}
\end{definition}

\begin{definition}[Variational Lower Bound - KL Divergence]	
	In order to reverse the forward process, we need that the forward process' distribution $q(\mathbf{x_{1:T}}|\mathbf{x_0})$ is as close to $p(\mathbf{x_{1:T}}|\mathbf{x_0})$ as possible. We can use the Kullback-Leibler (KL) divergence between $q$ and $p$ ($\mathbf{D_{KL}}$) to evaluate their difference as find a bound to $log [p_\theta(\mathbf{x}_0)]$. 
	\label{def:var_low_bound_kl_divergence}
	\begin{align}
		\mathbf{D_{KL}}\big[ q(\mathbf{x_{1:T} | x_0})  ||  p(\mathbf{x_{1:T} | x_0}) \big] & := \mathbb{E}_q\big[ log(q(\mathbf{x_{1:T} |x_0})  -  log(p(\mathbf{x_{1:T} |x_0})) \big] \\
		\text{using the Bayes' Rule we can write} & \\
		p(\mathbf{x_{1:T}|x_0}) &= \frac{p(\mathbf{x_{1:T}, x_0})}{p(\mathbf{x_0})} \\
		\implies 	\mathbf{D_{KL}}\big[ q(\mathbf{x_{1:T} | x_0})  ||  p(\mathbf{x_{1:T} | x_0}) \big] &= \mathbb{E}_q\big[ log(q(\mathbf{x_{1:T} | x_0})  -  log(p(\mathbf{x_{1:T} | x_0})) + log(p(\mathbf{x_0})) \big] \\
		\text{the prior of the latent variables does not depend on q} & \\
		\mathbf{D_{KL}}\big[ q(\mathbf{x_{1:T}| x_0})  ||  p(\mathbf{x_{1:T} | x_0}) \big] &= \mathbb{E}_q\big[ log(q(\mathbf{x_{1:T} | x_0})  -  log(p(\mathbf{x_{1:T}, x_0})) \big] + log(p(\mathbf{x_0})) \\
		\implies log(p(\mathbf{x_0})) = 	\mathbf{D_{KL}}\big[ q(\mathbf{x_{1:T} | x_0})  ||  p(\mathbf{x_{1:T} | x_0}) \big] & -   \mathbb{E}_q\big[ log(q(\mathbf{x_{1:T} | x_0})  -  log(p(\mathbf{x_{1:T}, x_0})) \big] \\
		log(p(\mathbf{x_0})) = 	\mathbf{D_{KL}}\big[ q(\mathbf{x_{1:T} | x_0})  ||  p(\mathbf{x_{1:T} | x_0}) \big] & +   \mathbb{E}_q\big[  log(p(\mathbf{x_{1:T}, x_0}) - log(q(\mathbf{x_{1:T} | x_0})) \big] \\
		\text{but   }	\mathbf{D_{KL}}\big[ q(\mathbf{x_{1:T} | x_0})  ||  p(\mathbf{x_{1:T} | x_0}) \big] & \geq 0 \\
		\implies  - log(p(\mathbf{x_0})) &\leq  \mathbb{E}_q\big[  - log\frac{p(\mathbf{x_{0:T})}}{q(\mathbf{x_{1:T} | x_0})} \big] \\
		\text{If we define the Variational Lower Bound L as:}  &\\
		L &:=   \mathbb{E}_q\bigg[log \frac{p(\mathbf{x_{0:T}})} {q(\mathbf{x_{1:T}| x_0})}\bigg] \\
		\implies - log [p(\mathbf{x_0})] &\leq - L
	\end{align}
\end{definition}

We have found lower bound $L$ for the log-likelihood function that is tractable if rewritten properly. Our goal is to have a process to sample from $p$ such that $log p(\mathbf{x_0})$ is as large as possible. This means that our process will output very likely outputs $\mathbf{x_0}$. In order to do so, DDPMs work in two steps: \textbf{noise prediction} and \textbf{sampling}. The former is the one responsible for predicting the a noise $\epsilon_\theta$ of an input $\mathbf{x_t}$ which has has been sampled from the forward process' distribution $q$. The latter uses such prediction to sample $\mathbf{x_0}$ from $p$.

\subsubsection{Noise Predictor Training Derivation}

Having ELBO ($L$) as a lower bound for the log likelihood function means we can train our noise predictor model to maximize $L$, i.e., $L$ is a candidate for our model's loss function. In order to do so, further algebraic manipulation must be performed with it in order to make it tractable in a way that the noise that has been added to the input appears somewhere. We demonstrate such manipulations here and will begin by showing that maximing $L$ basically means approximating the distributions $p(\mathbf{x_{t-1} | x_t}) $ and $q(\mathbf{x_{t-1} | x_t,  x_0})$.




\begin{definition}[Noise Predictor's Loss Derivation]
	In order to build the Noise Predictor's loss function, we need to remember that both forward and reverse processes are Markov Chains and use this fact to manipulate $L$.
	\begin{align}
		L & =   \mathbb{E}_q\bigg[log \frac{p(\mathbf{x_{0:T}})} {q(\mathbf{x_{1:T}| x_0})}\bigg] \\
		& \text{The forward and reverse processes are Markov Chains, so} \\
		 \mathbb{E}_q\bigg[ log \frac{p(\mathbf{x_{0:T}})} {q(\mathbf{x_{1:T}| x_0})} \bigg] &=  \mathbb{E}_q\bigg[ log \frac{p(\mathbf{x_{T}}) \prod_{t=1}^T p_\theta(\mathbf{x_{t-1} | x_t})} {q(\mathbf{x_1| x_0}) \prod_{t=2}^T q(\mathbf{x_{t} | x_{t-1}, x_0})} \bigg] \\
		&=  \mathbb{E}_q\bigg[ log \frac{p(\mathbf{x_{T}}) \prod_{t=1}^T p_\theta(\mathbf{x_{t-1} | x_t})} {q(\mathbf{x_T| x_0}) \prod_{t=2}^T q(\mathbf{x_{t-1} | x_t, x_0})} \bigg] \\
		&=  \mathbb{E}_q\bigg[ log \frac{p(\mathbf{x_{T}}) p_\theta(\mathbf{x_0 | x_1})\prod_{t=2}^T p_\theta(\mathbf{x_{t-1} | x_t})} {q(\mathbf{x_T| x_0}) \prod_{t=2}^T q(\mathbf{x_{t-1} | x_t, x_0})} \bigg] \\
		&=  \mathbb{E}_q\bigg[ log \frac{p(\mathbf{x_{T}})}{q(\mathbf{x_T| x_0})} \bigg] + log p_\theta(\mathbf{x_0 | x_1}) +  \sum_{t=2}^T  \mathbb{E}_q\bigg[ log  \frac{p_\theta(\mathbf{x_{t-1} | x_t})} {q(\mathbf{x_{t-1} | x_t, x_0})} \bigg] \\
		&=  \mathbb{E}_q\bigg[ log \frac{p(\mathbf{x_{T}})}{q(\mathbf{x_T| x_0})} \bigg] + log p_\theta(\mathbf{x_0 | x_1}) + \sum_{t=2}^T 	\mathbf{D_{KL}}\big[ p_\theta (\mathbf{x_{t-1} | x_t})  || q(\mathbf{x_{t-1} | x_t,  x_0}) \big]
	\end{align}
\end{definition}

If we pay attention to equation $41$'s terms above, we see that the first term is parameter free, because $p(\mathbf(x_T))$ is fixed and defined as a Gaussian, while $q(\mathbf(x_T | x_0))$ is also Gaussian from the definition of the forward process. Hence, we are left with the second and third terms. 

As previously mentioned, we are interested in maximizing $L$. Using the equation $41$, we see that doing so is equivalent to minimizing the KL Divergence between $p_\theta (\mathbf{x_{t-1} | x_t})$ and $q(\mathbf{x_{t-1} | x_t,  x_0})$. We know that both distributions are Gaussians, which makes computing the KL Divergence between them easier if we know their mean and variance. We will begin by calculating such moments for $q(\mathbf{x_{t-1} | x_t,  x_0})$.
	
\begin{align}
	 q(\mathbf{x_{t-1} | x_t,  x_0}) &= \frac{q(\mathbf{x_t | x_{t-1}, x_0}) q(\mathbf{x_{t-1}|x_0})}{q(\mathbf{x_t | x_0})} \\
	 & \text{we know the q distribution from Definition \ref{def:cum_noise}, hence } \\ 
	 q(\mathbf{x_{t-1} | x_t,  x_0}) &\text{    is a product of known Gaussians over another known Gaussian that lets us define} \\
	 \mu_q(\mathbf{x_t, x_0}) &= \frac{(1-\bar{\alpha}_{t-1})\sqrt{\alpha_t} \mathbf{x_t} + (1- \alpha_t) \sqrt{\bar{\alpha}_{t-1}  } \mathbf{x_0}}{(1-\bar{\alpha_t})} \\
	 \Sigma_q(t) &= \frac{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}  \mathbf{I} \\
	 & \implies  q(\mathbf{x_{t-1} | x_t,  x_0}) =   \mathcal{N}(\mathbf{x_{t-1}};  \mu_q(\mathbf{x_t, x_0}); \Sigma_q(t))
\end{align}

We have just defined the $q(\mathbf{x_{t-1} | x_t,  x_0})$ distribution. Now let's move on to the  $p_\theta (\mathbf{x_{t-1} | x_t})$ distribution.
	
\begin{align}
	p_\theta(\mathbf{x_{t-1} | x_t}) &=   \mathcal{N}(\mathbf{x_{t-1}};  \mu_\theta(\mathbf{x_t}); \Sigma_\theta(t)) \\
	& \text{the reverse process variance is defined as the ground truth variance of the forward process: } \\
 \Sigma_\theta(t) &= \Sigma_q(t) \\
	& \text{    we are only left with the distribution's mean   } \mu_\theta \\
	p_\theta(\mathbf{x_{t-1} | x_t}) &=   \mathcal{N}(\mathbf{x_{t-1}};  \mu_\theta(\mathbf{x_t}); \Sigma_q(t))
\end{align}

Equation 50 makes it much easier to calculate $\mathbf{D_{KL}}\big[ p_\theta (\mathbf{x_{t-1} | x_t})  || q(\mathbf{x_{t-1} | x_t,  x_0}) \big]$: now we know we are trying to compute the KL Divergence between two Gaussians with the exact same variance. For that,  there is the following result that arives from the definition of such divergence:

\begin{align}
	d_1(x) &= \mathcal{N}(\mu_1, \sigma^2)  \\
	d_2(x) &= \mathcal{N}(\mu_2, \sigma^2) \\
	\text{The KL divergence   } & D_{KL}(d_1 | d_2)  \text{   is given by:} \\
	D_{KL}(d_1 | d_2) &= \frac{(\mu_1 - \mu_2)^2}{2\sigma^2}\\
	&\text{Hence,  }\\
\mathbf{D_{KL}}\big[ p_\theta (\mathbf{x_{t-1} | x_t})  || q(\mathbf{x_{t-1} | x_t,  x_0}) \big] &=  \mathbf{D_{KL}}\big(   \mathcal{N}(\mathbf{x_{t-1}};  \mu_\theta(\mathbf{x_t}); \Sigma_q(t)), \mathcal{N}(\mathbf{x_{t-1}};  \mu_q(\mathbf{x_t, x_0}); \Sigma_q(t)) \big) \\
&=  \frac{1- \bar{\alpha}_t }{2(1-\alpha_t)(1- \bar\alpha_{t-1})} \big|\big| (\mu_\theta - \mu_q)^2_2\big|\big|
\end{align}

As our goal is to minimize the KL Divergence, from equation 59 we see that such goal comes down to basically minimizing the 
difference $\mu_\theta - \mu_q$, i.e., \textbf{we just need to minimize the difference between the means of the reverse and forward processes' distributions}. We need to define the reverse process' distribution mean prediction by taking a look at the forward process' one. 
\begin{align}
	 \mu_q(\mathbf{x_t, x_0}) &= \frac{(1-\bar{\alpha}_{t-1})\sqrt{\alpha_t} \mathbf{x_t} + (1- \alpha_t) \sqrt{\bar{\alpha}_{t-1}  } \mathbf{x_0}}{(1-\bar{\alpha_t})} \\
	 & \text{we can define the prediction }\\
	 \hat\mu_q(\mathbf{x_t, x_0}) &=  \mu_\theta(\mathbf{x_t}) \\
	 &= \frac{(1-\bar{\alpha}_{t-1})\sqrt{\alpha_t} \mathbf{x_t} + (1- \alpha_t) \sqrt{\bar{\alpha}_{t-1}  } \mathbf{x_\theta}}{(1-\bar{\alpha_t})}
\end{align}
	
In equation 63 we see that we are using our reverse process model's prediction $\mathbf{x_\theta}$ in the prediction of its distribution's mean, which let's us rewrite $\big|\big| (\mu_\theta - \mu_q)^2_2\big|\big|$ in terms of $\mathbf{x_0}$ and $\mathbf{x_\theta}$ which leves us with the following for the KL Divergence:
	
\begin{align}
	 \mathbf{D_{KL}}\big(   \mathcal{N}(\mathbf{x_{t-1}};  \mu_\theta(\mathbf{x_t}); \Sigma_q(t)), \mathcal{N}(\mathbf{x_{t-1}};  \mu_q(\mathbf{x_t, x_0}); \Sigma_q(t)) \big) &= \frac{(1- \bar{\alpha}_t)(\bar\alpha_{t-1}) }{2(1-\alpha_t)(1- \bar\alpha_{t-1})} \big|\big| (\mathbf{x_\theta - x_0})^2_2 \big|\big|
\end{align}
	
The author's of the DDPM paper mention that equation 64 can be used as the loss function to train the reverse process model. On the other hand, we now that the forward process actually predict the noise that was added to an input $\mathbf{x_t}$ intead of predicting $\mathbf{x_\theta}$ directly. This means that the loss function must account for the error prediction somehow. This is achieved by further analysing $\mathbf{x_0}$ and $\mathbf{x_\theta}$ and remembering how $\mathbf{x_\theta}$ was defined in the forward process. 
	
\begin{align}
	q(x_t|x_0) &:= \mathcal{N}(x_{t}; \sqrt{\bar{\alpha_t}} \times x_0; (1 - \bar{\alpha_t})\mathbb{I}) \\
	&\text{which  let's us write } \\
	\mathbf{x_t} &= \sqrt{\bar{\alpha_t}}  x_0 +  \sqrt{1 - \bar{\alpha_t}}\epsilon \\
	\implies \mathbf{x_0} &= \frac{\mathbf{x_t} -  \sqrt{1-\bar{\alpha_t}}\epsilon}{\sqrt{\bar\alpha_t}}\\
	\text{for a Standard Gaussian Noise  } &\epsilon. \\
	\text{We can now define our prediction   } \hat\epsilon& = \epsilon_\theta\\
	\mathbf{x_\theta}  &= \frac{\mathbf{x_t} -  \sqrt{1-\bar{\alpha_t}}\epsilon_\theta}{\sqrt{\bar\alpha_t}} \\
	\implies 
	\frac{(1- \bar{\alpha}_t)(\bar\alpha_{t-1}) }{2(1-\alpha_t)(1- \bar\alpha_{t-1})}\big|\big| (\mathbf{x_\theta - x_0})^2_2 \big|\big| &= 	\frac{(1- \bar{\alpha}_t)(\bar\alpha_{t-1}) }{2(1-\alpha_t)(1- \bar\alpha_{t-1})} \frac{(1-\alpha_t)^2}{(1-\bar\alpha_t)\alpha_t}\big|\big| (\mathbf{\epsilon_\theta- \epsilon})^2_2 \big|\big|
\end{align}

Even though equation 72 could be used directly as the loss function for the noise predictor, the DDPM paper authors mention that optimizing $\big|\big| (\mathbf{\epsilon_\theta- \epsilon})^2_2 \big|\big|$ without the scaling factor with the cumulative noise $\alpha_t$ is enough. Hence, we have finally defined a function to be minimized for the noise predictor training and hence write its algorithm.

\begin{algorithm} 
	\begin{algorithmic}[1]
		\State \textbf{repeat}
		\State $\mathbf{x_0 \sim q(x_0)}$ \Comment{Sample image from training set}
		\State $\mathbf{x_0 \sim Uniform(\{1, \hdots, T\})}$ \Comment{Sample the step of the Forward Process Markov Chain}
		\State $\mathbf{\epsilon \sim \mathcal{N}(0, I)}$ \Comment{Sample standard gaussian noise to be added to the input}
		\State $\mathbf{x_t = \sqrt{\bar{\alpha_t}}  x_0 +  \sqrt{1 - \bar{\alpha_t}}\epsilon }$ \Comment{Forward Process/ Generating Noisy Image}
		\State Take Gradient Descent Step on  $\nabla_\theta(\mathbf{|| \epsilon - \epsilon_\theta(x_t, t)|| })$
		\State \textbf{until} converged
	\end{algorithmic} 
	\caption{Noise Predictor Training}
	\label{alg:algorithm1}
\end{algorithm}

\subsubsection{Sampling Algorithm Derivation}


Now that we have defined a way to predict an input image $\mathbf{x_t}$'s noise, we need a way to use such prediction to reconstruct the original de-noised image $\mathbf{x_0}$, i.e., we need a way to sample from $p(\mathbf{x_0})$. To do so, let's recall how we have defined the $p_\theta (\mathbf{x_{t-1} | x_t})$ distribution.


\begin{align}
	p_\theta(\mathbf{x_{t-1} | x_t}) &=   \mathcal{N}(\mathbf{x_{t-1}};  \mu_\theta(\mathbf{x_t}); \Sigma_q(t)) \\
	 \mu_\theta(\mathbf{x_t}) 	 &= \frac{(1-\bar{\alpha}_{t-1})\sqrt{\alpha_t} \mathbf{x_t} + (1- \alpha_t) \sqrt{\bar{\alpha}_{t-1}  } \mathbf{x_\theta}}{(1-\bar{\alpha_t})}\\
	 \mathbf{x_\theta}  &= \frac{\mathbf{x_t} -  \sqrt{1-\bar{\alpha_t}}\epsilon_\theta}{\sqrt{\bar\alpha_t}} \\
	 \implies  \mu_\theta(\mathbf{x_t}) &= \frac{\mathbf{x_t}}{\sqrt{\alpha_t}} - \frac{(1-\alpha_t)(\sqrt{1-\bar\alpha_t})}{(1-\bar\alpha_t)(\sqrt{\alpha_t})} \epsilon_\theta  = \frac{1}{\sqrt{\alpha_t}}\bigg( \mathbf{x_t} - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha_t}}}\epsilon_\theta\bigg) \\
	 \Sigma_\theta (t) = \Sigma_q(t) &= \frac{(1-\alpha_t)(1-\bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}  \mathbf{I} \\
	 \text{We now have defined   } \mathbf{x_{t-1}} &\text{'s mean and variance given   } \mathbf{x_t} \text{  which let's us write it as } \\
	 \mathbf{x_{t-1}} &= \mu_\theta(\mathbf{x_t}) + \sqrt{\Sigma_\theta(t)} \mathbf{z} \\
	 &\mathbf{z \sim \mathcal{N}(0, I)}	 
\end{align}

We have now a way to generate $\mathbf{x_{t-1}}$ from $\mathbf{x_t}$. This means that if we have $\mathbf{x_1}$ we can generate $\mathbf{x_0}$. This let's us finally define our sampling algorithm:

\begin{algorithm} 
	\begin{algorithmic}[1]
		\State \textbf{repeat}
		\State $\mathbf{x_T \sim \mathcal{N}(0, I)}$ \Comment{Sample random noisy image}
		\State $\mathbf{T \sim Uniform(\{1, \hdots, 1000\})}$ \Comment{Sample random length of the Denoising Chain. Max chain size of 1000 was set arbitrarily }
		\State \textbf{for} $\mathbf{t =T, \hdots, 1}$ \textbf{do}
		\State $\mathbf{z \sim \mathcal{N}(0, I)}$ if $t > 1$ else $\mathbf{z = 0}$
		\State $\mathbf{x_{t-1}} =  \frac{1}{\sqrt{\alpha_t}}\bigg( \mathbf{x_t} - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha_t}}}\epsilon_\theta(\mathbf{x_t,t})\bigg) + \sqrt{\Sigma_\theta(t)} \mathbf{z} $  \Comment{Sampling  } $\mathbf{x_{t-1}}$ from $p_\theta (\mathbf{x_{t-1} | x_t})$
		\State \textbf{end for}
		\State \textbf{return} $\mathbf{x_0}$
	\end{algorithmic} 
	\caption{Sampling}
	\label{alg:sampling}
\end{algorithm} 


\subsection{Recap}

We have studied both the forward and reverse process that make up DDPMs. We have seen that good samples (or images) are generated by maximing the reverse process' likelihood $log p_\theta(\mathbf{x_0})$ lower bound, the Evidence Lower Bound, ELBO, or simply $L$. 

We have rewritten ELBO in a way that its maximization turns out to be dual with minimizing the Kullback-Leibler (KL) divergence between  $p_\theta (\mathbf{x_{t-1} | x_t})$ and $q(\mathbf{x_{t-1} | x_t,  x_0})$. Such KL-Divergence minimization was then translated to a noise predictor training. After the predictor training algorithm was defined, we have shown how to use such prediction to sample from  $p_\theta (\mathbf{x_{t-1} | x_t})$ and finally reconstructing $\mathbf{x_0}$, the original denoised image.


\bibliographystyle{plain}
\bibliography{references}

%----------------------------------------------------------------------------------------

\end{document}
